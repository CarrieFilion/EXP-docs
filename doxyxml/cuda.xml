<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.8" xml:lang="en-US">
  <compounddef id="cuda" kind="page">
    <compoundname>cuda</compoundname>
    <title>CUDA implementation</title>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<sect1 id="cuda_1overview">
<title>Overview</title>
<para>GPU computation works best when the CPU<ndash/>GPU data transfer is minimal. One could design the GPU implementation so that all computations<ndash/>drift, kick, and force evaluation<ndash/>is computed on the GPU. Recent versions of MPI implementation allow for GPU to GPU data transfer that bypass CPU staging that would allow such an implementation. However, the user also wants to accumulate phase space information, and this requires transfer between the CPU and GPU.</para>
<para>Also, in nearly all cases, the per time step bottleneck is the basis construction and force evaluation by at least an order of magnitude. So rather than fully re-implement EXP on the GPU, this implementation only computes the basis and force on the GPU. At this point, only two basis classes have GPU implementation, <ref refid="class_spherical_basis" kindref="compound">SphericalBasis</ref> and <ref refid="class_cylinder" kindref="compound">Cylinder</ref>. Both of these assume that the basis functions (either one- or two-dimensional) are stored in tables which require special treatment (see <ref refid="cuda_1strategy" kindref="member">Strategy</ref> below). The other analytic and recursion bases and the user modules could be similarly and probably more easily implemented on the GPU. I am leaving these implementations for a later date, as needed.</para>
</sect1>
<sect1 id="cuda_1strategy">
<title>Strategy</title>
<para>The autoconf tools will automatically detect the presense of the CUDA toolkit and set a config flag which enables the compilation of the GPU implementation. GPU availability is probed at runtime. Each GPU is assigned to a single thread on a single MPI process. The multistep coefficient update has too much divergence for a fast GPU implementation. However, one can speed up coefficient update relative to the GPU coefficient and force evaluation by assigning the remaining cores on the node to threads using the <computeroutput>nthrd</computeroutput> global parameter (see <ref refid="cuda_1use" kindref="member">Usage notes</ref> below).</para>
<para>The <ref refid="class_component" kindref="compound">Component</ref> class assigns the GPU by device number to the process on instantiation. When the associated <ref refid="class_pot_accel" kindref="compound">PotAccel</ref> method is instantiated, a valid device number triggers initialization of the GPU data space. These data include phase space, coefficient, and basis table structures. The basis tables, which are constant data, are copied to the GPU during initialization.</para>
</sect1>
<sect1 id="cuda_1impl">
<title>Implementation details</title>
<para>The GPU computation is done in multiple steps: <orderedlist>
<listitem>
<para>EXP partitions the <formula id="68">$N$</formula> particles between the <formula id="69">$n$</formula> nodes uniformly or according to work-load factors by EXP. Let us assume that a particular process has <formula id="70">$N_c$</formula> particles.</para>
<para></para>
</listitem>
<listitem>
<para>The phase space with <formula id="68">$N$</formula> particles is partitioned in bunches of size <formula id="71">$M&lt;N_c$</formula>. The value of <formula id="72">$M$</formula> is user adjustable. The break-even bunch size (the size beyond which the scaling is linear) is approximately <formula id="73">$M=10^6$</formula>.</para>
<para></para>
</listitem>
<listitem>
<para>The native coordinates for the particular basis (e.g. spherical or cylindrical coordinates) are evaluated for the particles whose coefficients are to be evaluated using a specialized kernel.</para>
<para></para>
</listitem>
<listitem>
<para>The coefficient contributions are evaluated using a specialized kernel and staged in GPU memory.</para>
<para></para>
</listitem>
<listitem>
<para>The coefficients for each evaluated particle are tree-reduced on the GPU and returned to the CPU.</para>
<para></para>
</listitem>
<listitem>
<para>The CPU sum reduces these contributions using MPI and pushes them back to the GPU.</para>
<para></para>
</listitem>
<listitem>
<para>The force is computed for all required particles (i.e. all particle at the current multistep which could be all or a subset of <formula id="70">$N_c$</formula>) using the coefficients and specialize kernel.</para>
<para></para>
</listitem>
<listitem>
<para>The particle phase space including the new acceleration values are returned to the CPU from the GPU.</para>
<para></para>
</listitem>
</orderedlist>
</para>
<para>The coefficient table data is packed as CUDA textures and the table interpolation for the coefficient values use the native CUDA texture routines. These are much faster than hand-coded interpolation methods. This is especially true for the two-dimensional table lookup needed for the <ref refid="class_cylinder" kindref="compound">Cylinder</ref> methods. All interpolation is linear and the tables are constructed with sufficiently small grid spacing that this is not a numerical limitation.</para>
</sect1>
<sect1 id="cuda_1use">
<title>Usage notes</title>
<para><orderedlist>
<listitem>
<para>Start one process for each GPU on each node </para>
</listitem>
<listitem>
<para>Divide the CPU cores among the processes per node. For example, if one has 40 cores and 4 GPUs, set <computeroutput><ndash/>npernode 4</computeroutput> in your <computeroutput>mpirun</computeroutput> command and set <computeroutput>nthrd=10</computeroutput> in the EXP config file. </para>
</listitem>
</orderedlist>
</para>
</sect1>
    </detaileddescription>
    <location file="cuda.doc"/>
  </compounddef>
</doxygen>
